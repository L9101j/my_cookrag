#LLMç”Ÿæˆæ¨¡å—æ„å»º
import logging
from typing import List, Dict, Any
from langchain.chat_models import init_chat_model
from langchain.agents import create_agent
from langchain_core.documents import Document
from langchain_core.messages import AIMessageChunk
from ..db.database import Recipe
import os
from ..config import settings

logger = logging.getLogger(__name__)
class RecipeLLMGeneration:
    """èœè°±LLMç”Ÿæˆå™¨"""
    def __init__(self, model_name: str):
        self.model_name = "deepseek-chat" if model_name == "DEEPSEEK" else None
        self.model_provider = "deepseek" if model_name == "DEEPSEEK" else None
        self.llm = None
        self.setup_llm()

    def setup_llm(self):
        """è®¾ç½®LLMæ¨¡å‹"""
        os.environ["DEEPSEEK_API_KEY"] = settings.DEEPSEEK_API_KEY
        self.llm = init_chat_model(
            model=self.model_name,
            model_provider=self.model_provider
        )

    def query_router(self, query: str):
        """æŸ¥è¯¢è·¯ç”±"""
        classification_prompt = f"""
        ä½ æ˜¯ä¸€ä¸ªèœè°±é—®ç­”åˆ†ç±»å™¨ã€‚è¯·æ ¹æ®ç”¨æˆ·é—®é¢˜åˆ¤æ–­å…¶æ„å›¾ï¼Œåªèƒ½è¾“å‡ºä»¥ä¸‹ä¸‰ä¸ªæ ‡ç­¾ä¹‹ä¸€ï¼š
        - listï¼šå½“ç”¨æˆ·æƒ³è¦è·å–èœå“åˆ—è¡¨æˆ–æ¨èï¼ˆåªéœ€è¦èœåï¼‰ï¼Œå¦‚â€œæ¨èå‡ ä¸ªç´ èœâ€â€œæœ‰ä»€ä¹ˆå·èœâ€â€œç»™æˆ‘3ä¸ªç®€å•çš„èœâ€ã€‚
        - detailï¼šå½“ç”¨æˆ·æƒ³è¦å…·ä½“çš„åˆ¶ä½œæ–¹æ³•ã€æ­¥éª¤æˆ–æ‰€éœ€é£Ÿæï¼Œå¦‚â€œå®«ä¿é¸¡ä¸æ€ä¹ˆåšâ€â€œéœ€è¦å“ªäº›ææ–™â€â€œåˆ¶ä½œæ­¥éª¤æ˜¯ä»€ä¹ˆâ€ã€‚
        - generalï¼šå…¶ä»–æ³›åŒ–æˆ–èƒŒæ™¯ç±»é—®é¢˜ï¼Œå¦‚â€œä»€ä¹ˆæ˜¯å·èœâ€â€œçƒ¹é¥ªæŠ€å·§â€â€œè¥å…»ä»·å€¼â€ã€‚

        è¯·æ³¨æ„ï¼š
        1. åªè¿”å›æ ‡ç­¾æœ¬èº«ï¼Œä¸è¦å¤šå†™ä»»ä½•æ–‡å­—ã€‚
        2. å¦‚æœé—®é¢˜åŒæ—¶åŒ…å«å¤šä¸ªæ„å›¾ï¼Œä»¥ç”¨æˆ·æœ€ä¸»è¦çš„éœ€æ±‚åˆ¤æ–­ã€‚
        3. è‹¥é—®é¢˜æ— æ³•ç¡®å®šï¼Œè¿”å› generalã€‚

        ç”¨æˆ·é—®é¢˜ï¼š{query}
        æ ‡ç­¾ï¼š"""
        
        classification_chain = create_agent(
            model=self.llm,
            tools=[]
        )
        result = classification_chain.invoke({"messages": [{"role": "user", "content": classification_prompt}]})
        return result


    def general_question(self,query:str,context:str,streaming:bool=False):
        """é€šç”¨é—®é¢˜"""
        general_prompt = f"""
            ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„çƒ¹é¥ªåŠ©æ‰‹ã€‚è¯·æ ¹æ®ä»¥ä¸‹é£Ÿè°±ä¿¡æ¯å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

            ç”¨æˆ·é—®é¢˜: {query}

            ç›¸å…³é£Ÿè°±ä¿¡æ¯:
            {context}

            è¯·æä¾›è¯¦ç»†ã€å®ç”¨çš„å›ç­”ã€‚å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œè¯·è¯šå®è¯´æ˜ã€‚

            å›ç­”:
            """
        
        general_chain = create_agent(
            model=self.llm,
            tools=[]
        )
        if streaming:
            for token, meta in general_chain.stream({"messages": [{"role": "user", "content": general_prompt}]},
                                              stream_mode="messages"
                                              ):
                if isinstance(token, AIMessageChunk):
                    yield token.content
        else:
            return general_chain.invoke({"messages": [{"role": "user", "content": general_prompt}]})
    
    
    def rewrite_query(self,query:str):
        prompt = f"""
        ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½æŸ¥è¯¢åˆ†æåŠ©æ‰‹ã€‚è¯·åˆ†æç”¨æˆ·çš„æŸ¥è¯¢ï¼Œåˆ¤æ–­æ˜¯å¦éœ€è¦é‡å†™ä»¥æé«˜é£Ÿè°±æœç´¢æ•ˆæœã€‚

        åŸå§‹æŸ¥è¯¢: {query}

        åˆ†æè§„åˆ™ï¼š
        1. **å…·ä½“æ˜ç¡®çš„æŸ¥è¯¢**ï¼ˆç›´æ¥è¿”å›åŸæŸ¥è¯¢ï¼‰ï¼š
        - åŒ…å«å…·ä½“èœå“åç§°ï¼šå¦‚"å®«ä¿é¸¡ä¸æ€ä¹ˆåš"ã€"çº¢çƒ§è‚‰çš„åˆ¶ä½œæ–¹æ³•"
        - æ˜ç¡®çš„åˆ¶ä½œè¯¢é—®ï¼šå¦‚"è›‹ç‚’é¥­éœ€è¦ä»€ä¹ˆé£Ÿæ"ã€"ç³–é†‹æ’éª¨çš„æ­¥éª¤"
        - å…·ä½“çš„çƒ¹é¥ªæŠ€å·§ï¼šå¦‚"å¦‚ä½•ç‚’èœä¸ç²˜é”…"ã€"æ€æ ·è°ƒåˆ¶ç³–é†‹æ±"

        2. **æ¨¡ç³Šä¸æ¸…çš„æŸ¥è¯¢**ï¼ˆéœ€è¦é‡å†™ï¼‰ï¼š
        - è¿‡äºå®½æ³›ï¼šå¦‚"åšèœ"ã€"æœ‰ä»€ä¹ˆå¥½åƒçš„"ã€"æ¨èä¸ªèœ"
        - ç¼ºä¹å…·ä½“ä¿¡æ¯ï¼šå¦‚"å·èœ"ã€"ç´ èœ"ã€"ç®€å•çš„"
        - å£è¯­åŒ–è¡¨è¾¾ï¼šå¦‚"æƒ³åƒç‚¹ä»€ä¹ˆ"ã€"æœ‰é¥®å“æ¨èå—"

        é‡å†™åŸåˆ™ï¼š
        - ä¿æŒåŸæ„ä¸å˜
        - å¢åŠ ç›¸å…³çƒ¹é¥ªæœ¯è¯­
        - ä¼˜å…ˆæ¨èç®€å•æ˜“åšçš„
        - ä¿æŒç®€æ´æ€§

        ç¤ºä¾‹ï¼š
        - "åšèœ" â†’ "ç®€å•æ˜“åšçš„å®¶å¸¸èœè°±"
        - "æ¨èä¸ªèœ" â†’ "ç®€å•å®¶å¸¸èœæ¨è"
        - "å·èœ" â†’ "ç»å…¸å·èœèœè°±"
        - "å®«ä¿é¸¡ä¸æ€ä¹ˆåš" â†’ "å®«ä¿é¸¡ä¸æ€ä¹ˆåš"ï¼ˆä¿æŒåŸæŸ¥è¯¢ï¼‰
        - "çº¢çƒ§è‚‰éœ€è¦ä»€ä¹ˆé£Ÿæ" â†’ "çº¢çƒ§è‚‰éœ€è¦ä»€ä¹ˆé£Ÿæ"ï¼ˆä¿æŒåŸæŸ¥è¯¢ï¼‰

        è¯·è¾“å‡ºæœ€ç»ˆæŸ¥è¯¢ï¼ˆå¦‚æœä¸éœ€è¦é‡å†™å°±è¿”å›åŸæŸ¥è¯¢ï¼‰:"""
        rewrite_chain = create_agent(
            model=self.llm,
            tools=[]
        )
        
        return rewrite_chain.invoke({"messages": [{"role": "user", "content": prompt}]})



    def detail_question(self,query:str,context:str,streaming:bool=False):
        """è¯¦ç»†é—®é¢˜"""
        detail_prompt = f"""
            ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„çƒ¹é¥ªå¯¼å¸ˆã€‚è¯·æ ¹æ®é£Ÿè°±ä¿¡æ¯ï¼Œä¸ºç”¨æˆ·æä¾›è¯¦ç»†çš„åˆ†æ­¥éª¤æŒ‡å¯¼ã€‚

            ç”¨æˆ·é—®é¢˜: {query}

            ç›¸å…³é£Ÿè°±ä¿¡æ¯:
            {context}

            è¯·çµæ´»ç»„ç»‡å›ç­”ï¼Œå»ºè®®åŒ…å«ä»¥ä¸‹éƒ¨åˆ†ï¼ˆå¯æ ¹æ®å®é™…å†…å®¹è°ƒæ•´ï¼‰ï¼š

            ## ğŸ¥˜ èœå“ä»‹ç»
            [ç®€è¦ä»‹ç»èœå“ç‰¹ç‚¹å’Œéš¾åº¦]

            ## ğŸ›’ æ‰€éœ€é£Ÿæ
            [åˆ—å‡ºä¸»è¦é£Ÿæå’Œç”¨é‡]

            ## ğŸ‘¨â€ğŸ³ åˆ¶ä½œæ­¥éª¤
            [è¯¦ç»†çš„åˆ†æ­¥éª¤è¯´æ˜ï¼Œæ¯æ­¥åŒ…å«å…·ä½“æ“ä½œå’Œå¤§æ¦‚æ‰€éœ€æ—¶é—´]

            ## ğŸ’¡ åˆ¶ä½œæŠ€å·§
            [ä»…åœ¨æœ‰å®ç”¨æŠ€å·§æ—¶åŒ…å«ã€‚å¦‚æœåŸæ–‡çš„"é™„åŠ å†…å®¹"ä¸çƒ¹é¥ªæ— å…³æˆ–ä¸ºç©ºï¼Œå¯ä»¥åŸºäºåˆ¶ä½œæ­¥éª¤æ€»ç»“å…³é”®è¦ç‚¹ï¼Œæˆ–è€…å®Œå…¨çœç•¥æ­¤éƒ¨åˆ†]

            æ³¨æ„ï¼š
            - æ ¹æ®å®é™…å†…å®¹çµæ´»è°ƒæ•´ç»“æ„
            - ä¸è¦å¼ºè¡Œå¡«å……æ— å…³å†…å®¹
            - é‡ç‚¹çªå‡ºå®ç”¨æ€§å’Œå¯æ“ä½œæ€§

            å›ç­”:
            """
        
        detail_chain = create_agent(
            model=self.llm,
            tools=[]
        )
        if streaming:
            for token, meta in detail_chain.stream({"messages": [{"role": "user", "content": detail_prompt}]},
                                             stream_mode="messages"
                                              ):
                if isinstance(token, AIMessageChunk):
                    yield token.content
        else:
            return detail_chain.invoke({"messages": [{"role": "user", "content": detail_prompt}]})


    def list_question(self,query:str,context_docs:List[Recipe]):
        if not context_docs:
            return "æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ç›¸å…³é£Ÿè°±"
        dish_names = []
        for doc in context_docs:
            dish_name = doc.name if hasattr(doc, "name") else "æœªçŸ¥"
            if dish_name not in dish_names:
                dish_names.append(dish_name)
        if len(dish_names) == 1:
            return f"ä¸ºæ‚¨æ¨èï¼š{dish_names[0]}"
        else:
            return f"ä¸ºæ‚¨æ¨èä»¥ä¸‹èœå“ï¼š\n" + "\n".join([f"{i+1}. {name}" for i, name in enumerate(dish_names)])
        

    def build_context(self,docs:List[Recipe],maxlength:int=2000)->str:
        """æ„å»ºä¸Šä¸‹æ–‡"""
        if not docs:
            logger.warning("æ²¡æœ‰å¯æ„å»ºä¸Šä¸‹æ–‡çš„æ–‡æ¡£")
            return ""
        context_parts = []
        context_length = 0
        for i, doc in enumerate(docs):
            # metadata = doc.metadata
            # metadata_info = f"é£Ÿè°±{i}:"
            # if "name" in metadata:
            #     metadata_info += f"èœå: {metadata['name']}\n"
            # if "category" in metadata:
            #     metadata_info += f"åˆ†ç±»: {metadata['category']}\n"
            # if "difficulty" in metadata:
            #     metadata_info += f"éš¾åº¦: {metadata['difficulty']}\n"
            metadata_info = f"é£Ÿè°±{i}: èœå: {doc.name}, åˆ†ç±»: {doc.category}, éš¾åº¦: {doc.difficulty}\n"
            doc_text = f"metadata_info:{metadata_info}\n{doc.content}"
            if context_length + len(doc_text) <= maxlength:
                context_parts.append(doc_text)
                context_length += len(doc_text)
            else:
                break
        return "\n".join(context_parts)

if __name__ == "__main__":
    llm_generator = RecipeLLMGeneration(model_name="DEEPSEEK")
    query = "å®«ä¿é¸¡ä¸æ€ä¹ˆåš"
    context = "å®«ä¿é¸¡ä¸æ˜¯ä¸€é“ä¸­å›½ä¼ ç»Ÿçš„åèœï¼Œä¸»è¦ç”±é¸¡è‚‰ã€å§œã€è’œã€æ–™é…’ã€ç›ã€å‘³ç²¾ã€æ²¹ç»„æˆã€‚"
    rewrite_result = llm_generator.rewrite_query(query)
    print("é‡å†™ç»“æœ:", rewrite_result)
    # ä» agent å“åº”ä¸­æå–æ–‡æœ¬å†…å®¹
    rewrite_query = rewrite_result['messages'][-1].content if 'messages' in rewrite_result else str(rewrite_result)
    print("é‡å†™åçš„æŸ¥è¯¢:", rewrite_query)
    
    intent_result = llm_generator.query_router(rewrite_query)
    print("æ„å›¾è¯†åˆ«ç»“æœ:", intent_result)
    # ä» agent å“åº”ä¸­æå–æ–‡æœ¬å†…å®¹
    intent = intent_result['messages'][-1].content.strip() if 'messages' in intent_result else str(intent_result).strip()
    print("æ„å›¾:", intent)
    
    if intent == "detail":
        result = llm_generator.detail_question(rewrite_query, context,streaming=True)
        for token,meta_data in result:
            if isinstance(token, AIMessageChunk):
                print(token.content, end="", flush=True)



